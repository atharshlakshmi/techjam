{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c9bd64",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed9a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                   cleaned_review_text  rating  review_length  \\\n",
       " 0    andrea is amazing our dog loves her and she al...       5              9   \n",
       " 1    andrea does a wonderful  job  with our wild pr...       5             11   \n",
       " 2                                    never called back       1              1   \n",
       " 3                          they dont answer the phones       3              3   \n",
       " 4                   limited information on the website       3              3   \n",
       " ..                                                 ...     ...            ...   \n",
       " 995                          brett williams is awesome       5              3   \n",
       " 996  they have they the kinds of cars a teen will n...       5              9   \n",
       " 997  i would not go back there for nothing the gent...       1             10   \n",
       " 998                                     awesome prices       5              2   \n",
       " 999                           not the best but alright       2              2   \n",
       " \n",
       "     sentiment  all_caps_ratio  relevancy_score  \n",
       " 0    POSITIVE        0.000000               48  \n",
       " 1    POSITIVE        0.000000               47  \n",
       " 2    POSITIVE        0.000000               41  \n",
       " 3    NEGATIVE        0.000000               42  \n",
       " 4    NEGATIVE        0.000000               42  \n",
       " ..        ...             ...              ...  \n",
       " 995  POSITIVE        0.000000               41  \n",
       " 996  NEGATIVE        0.000000               48  \n",
       " 997  NEGATIVE        0.043478               61  \n",
       " 998  POSITIVE        0.000000               40  \n",
       " 999  POSITIVE        0.000000               42  \n",
       " \n",
       " [1000 rows x 6 columns],\n",
       "                                    cleaned_review_text  rating  review_length  \\\n",
       " 0    i ordered a number of baby and maternity items...       5             22   \n",
       " 1                                      prices too high       3              2   \n",
       " 2    perfect activity for a perfect day  the crowds...       5             10   \n",
       " 3    clean  modern house \\nfresh flowers everywhere...       5             12   \n",
       " 4    excellent option and easier access to tijuanas...       5              6   \n",
       " ..                                                 ...     ...            ...   \n",
       " 195  translated by google very happy with my first ...       5            198   \n",
       " 196  fantastic food and staff it was very busy when...       5             12   \n",
       " 197                       awesome food from far places       5              4   \n",
       " 198  well honestly i should give  instead of  but i...       4             33   \n",
       " 199                      history man plus architecture       5              4   \n",
       " \n",
       "     sentiment  all_caps_ratio  relevancy_score  \n",
       " 0    POSITIVE        0.069767               59  \n",
       " 1    NEGATIVE        0.000000               41  \n",
       " 2    POSITIVE        0.000000               47  \n",
       " 3    POSITIVE        0.000000               46  \n",
       " 4    POSITIVE        0.000000               43  \n",
       " ..        ...             ...              ...  \n",
       " 195  NEGATIVE        0.003650              100  \n",
       " 196  POSITIVE        0.000000               53  \n",
       " 197  POSITIVE        0.000000               42  \n",
       " 198  POSITIVE        0.036585               80  \n",
       " 199  POSITIVE        0.000000               41  \n",
       " \n",
       " [200 rows x 6 columns],\n",
       " 0      Irrelevant\n",
       " 1      Irrelevant\n",
       " 2            Rant\n",
       " 3            Rant\n",
       " 4      Irrelevant\n",
       "           ...    \n",
       " 995    Irrelevant\n",
       " 996    Irrelevant\n",
       " 997          Rant\n",
       " 998    Irrelevant\n",
       " 999    Irrelevant\n",
       " Name: label, Length: 1000, dtype: object,\n",
       " 0      Valid\n",
       " 1       Rant\n",
       " 2      Valid\n",
       " 3      Valid\n",
       " 4      Valid\n",
       "        ...  \n",
       " 195    Valid\n",
       " 196    Valid\n",
       " 197    Valid\n",
       " 198    Valid\n",
       " 199    Valid\n",
       " Name: label, Length: 200, dtype: object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Insert Qwen labelled data\n",
    "train_data = pd.read_csv(\"../data/with_features/qwen_labelled_combined_reviews_with_features.csv\")\n",
    "\n",
    "# Insert hand labelled data\n",
    "test_data = pd.read_csv(\"../data/with_features/hand_labelled_combined_reviews_with_features.csv\")\n",
    "\n",
    "X_train= train_data[[\"review_text\", \"rating\", \"review_length\",\"sentiment\", \"all_caps_ratio\",\"relevancy_score\"]]\n",
    "y_train= train_data[\"label\"]\n",
    "\n",
    "X_test=test_data[[\"review_text\", \"rating\", \"review_length\",\"sentiment\", \"all_caps_ratio\",\"relevancy_score\"]]\n",
    "y_test=test_data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a389c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Turn words into numbers\u001b[39;00m\n\u001b[32m      2\u001b[39m tfidf = TfidfVectorizer(max_features=\u001b[32m5000\u001b[39m, stop_words=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_train_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcleaned_review_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m X_test_tfidf = tfidf.transform(X_test[\u001b[33m\"\u001b[39m\u001b[33mcleaned_review_text\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(X_train_tfidf.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:99\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     doc = \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    101\u001b[39m     doc = analyzer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:232\u001b[39m, in \u001b[36m_VectorizerMixin.decode\u001b[39m\u001b[34m(self, doc)\u001b[39m\n\u001b[32m    229\u001b[39m     doc = doc.decode(\u001b[38;5;28mself\u001b[39m.encoding, \u001b[38;5;28mself\u001b[39m.decode_error)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np.nan:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[31mValueError\u001b[39m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Turn words into numbers\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=(1,2))\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train[\"review_text\"])\n",
    "X_test_tfidf = tfidf.transform(X_test[\"review_text\"])\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71bc7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5005)\n",
      "(200, 5005)\n"
     ]
    }
   ],
   "source": [
    "# Numeric features \n",
    "\n",
    "# Store as numeric arrays\n",
    "numeric_cols = [\"rating\", \"review_length\",\"sentiment\", \"all_caps_ratio\",\"relevancy_score\"]\n",
    "X_train_numeric = X_train[numeric_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "X_test_numeric  = X_test[numeric_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "\n",
    "# Combine TF-IDF + numeric\n",
    "X_train_final = hstack([X_train_tfidf, X_train_numeric])\n",
    "X_test_final = hstack([X_test_tfidf, X_test_numeric])\n",
    "\n",
    "print(X_train_final.shape)\n",
    "print(X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e53ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          Ad       0.20      0.20      0.20        10\n",
      "  Irrelevant       0.10      0.71      0.18         7\n",
      "        Rant       0.79      0.79      0.79        14\n",
      "       Valid       0.94      0.70      0.81       169\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.51      0.60      0.49       200\n",
      "weighted avg       0.87      0.69      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_final)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10eeea03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m      2\u001b[39m new_reviews = pd.DataFrame([\n\u001b[32m      3\u001b[39m     {\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreview_text\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mThis app is amazing! It helped me organize all my tasks efficiently.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     },\n\u001b[32m     43\u001b[39m ])\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Transform inputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m new_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_reviews\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreview_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m new_numeric = new_reviews[[\u001b[33m\"\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreview_length\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mall_caps_ratio\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mrelevancy_score\u001b[39m\u001b[33m\"\u001b[39m]].values\n\u001b[32m     49\u001b[39m new_final = hstack([new_tfidf, new_numeric])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:2128\u001b[39m, in \u001b[36mTfidfVectorizer.transform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   2111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[32m   2112\u001b[39m \n\u001b[32m   2113\u001b[39m \u001b[33;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2124\u001b[39m \u001b[33;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[32m   2125\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2126\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg=\u001b[33m\"\u001b[39m\u001b[33mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tfidf.transform(X, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1418\u001b[39m, in \u001b[36mCountVectorizer.transform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIterable over raw text documents expected, string object received.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1417\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[32m   1421\u001b[39m _, X = \u001b[38;5;28mself\u001b[39m._count_vocab(raw_documents, fixed_vocab=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/techjam/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:501\u001b[39m, in \u001b[36m_VectorizerMixin._check_vocabulary\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_vocabulary()\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fixed_vocabulary_:\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[33m\"\u001b[39m\u001b[33mVocabulary not fitted or provided\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vocabulary_) == \u001b[32m0\u001b[39m:\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mVocabulary is empty\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotFittedError\u001b[39m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "# Insert new reviews if we want to show how it works\n",
    "new_reviews = pd.DataFrame([\n",
    "    {\n",
    "        \"review_text\": \"This app is amazing! It helped me organize all my tasks efficiently.\",\n",
    "        \"cleaned_review_text\": \"app amazing helped organize tasks efficiently\",\n",
    "        \"rating\": 5,\n",
    "        \"review_length\": 10,\n",
    "        \"sentiment\": 1,  # positive\n",
    "        \"all_caps_ratio\": 0,\n",
    "        \"relevancy_score\": 0.9,\n",
    "        \"label\": 4  # Valid\n",
    "    },\n",
    "    {\n",
    "        \"review_text\": \"Buy the new SuperWidget now! Limited stock, get yours today!\",\n",
    "        \"cleaned_review_text\": \"buy new superwidget limited stock get yours today\",\n",
    "        \"rating\": 5,\n",
    "        \"review_length\": 10,\n",
    "        \"sentiment\": 1,\n",
    "        \"all_caps_ratio\": 0,\n",
    "        \"relevancy_score\": 0.95,\n",
    "        \"label\": 1  # Ad\n",
    "    },\n",
    "    {\n",
    "        \"review_text\": \"I waited 30 minutes and nobody helped me. Terrible service!\",\n",
    "        \"cleaned_review_text\": \"waited 30 minutes nobody helped terrible service\",\n",
    "        \"rating\": 1,\n",
    "        \"review_length\": 10,\n",
    "        \"sentiment\": -1,\n",
    "        \"all_caps_ratio\": 0,\n",
    "        \"relevancy_score\": 0.8,\n",
    "        \"label\": 3  # Rant\n",
    "    },\n",
    "    {\n",
    "        \"review_text\": \"I saw some random fact about dolphins today, nothing to do with this app.\",\n",
    "        \"cleaned_review_text\": \"saw random fact dolphins today nothing do app\",\n",
    "        \"rating\": 3,\n",
    "        \"review_length\": 12,\n",
    "        \"sentiment\": 0,\n",
    "        \"all_caps_ratio\": 0,\n",
    "        \"relevancy_score\": 0.2,\n",
    "        \"label\": 2  # Irrelevant\n",
    "    },\n",
    "])\n",
    "\n",
    "\n",
    "# Transform inputs\n",
    "new_tfidf = tfidf.transform(new_reviews[\"review_text\"])\n",
    "new_numeric = new_reviews[[\"rating\", \"review_length\",\"sentiment\", \"all_caps_ratio\",\"relevancy_score\"]].values\n",
    "new_final = hstack([new_tfidf, new_numeric])\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(new_final)\n",
    "\n",
    "# Compare true vs predicted\n",
    "for review, true_label, pred in zip(new_reviews[\"text\"], new_reviews[\"label\"], predictions):\n",
    "    print(f\"Review: {review}\\n   True: {true_label} --> Predicted: {pred}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(new_reviews[\"label\"], predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
